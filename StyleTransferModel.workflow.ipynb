{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hHABTKyx7w0p","trusted":true},"outputs":[],"source":["import os\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","import torchvision.transforms.v2 as tt\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import cv2\n","from tqdm.notebook import tqdm\n","from torchvision.utils import save_image\n","from torchvision.utils import make_grid\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pickle\n","import PIL\n","import math\n","from IPython import display\n","import pandas as pd\n","import requests\n","from PIL import Image\n","from io import BytesIO\n","import os.path\n","from torch import linalg as LA\n","%matplotlib inline\n","\n","sns.set(style='darkgrid', font_scale=1.2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install onnx\n","!pip install onnxscript"]},{"cell_type":"markdown","metadata":{},"source":["# Enviroment Variables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coco_dataset_path = \"/kaggle/input/coco-2017-dataset/coco2017/train2017\"\n","style_image_url = \"https://uploads4.wikiart.org/00142/images/vincent-van-gogh/the-starry-night.jpg!Large.jpg\"\n","style_representation_file_path = \"style.jpg\"\n","model_file_path = \"model.onnx\""]},{"cell_type":"markdown","metadata":{},"source":["# Data Preporation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class StyleDataset(torch.utils.data.Dataset):\n","    def __init__(self, urls):\n","        super(StyleDataset, self).__init__()\n","        \n","        self.urls = urls\n","        \n","    def __getitem__(self, index):\n","        response = requests.get(self.urls[index])\n","        img = Image.open(BytesIO(response.content))\n","        return transforms(img)\n","    \n","    def __len__(self):\n","        return len(self.urls)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ContentDataset(torch.utils.data.Dataset):\n","    def __init__(self, root_path, transforms):\n","        super(ContentDataset, self).__init__()\n","        \n","        self.paths = []\n","        self.transforms = transforms\n","\n","        for root, dirs, files in os.walk(root_path):\n","            for file in tqdm(files):\n","                self.paths.append(os.path.join(root, file))\n","        \n","    def __getitem__(self, index):\n","        img = Image.open(self.paths[index])\n","        return self.transforms(img)\n","    \n","    def __len__(self):\n","        return len(self.paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean=[0.485, 0.456, 0.406]\n","std=[0.229, 0.224, 0.225]\n","img_size = 256\n","\n","transforms = tt.Compose([\n","    tt.ToImage(),\n","    tt.ToDtype(dtype=torch.float32, scale=True),\n","    tt.Normalize(mean=mean, std=std)\n","])\n","\n","transforms2 = tt.Compose([\n","    tt.ToImage(),\n","    tt.Resize(img_size, antialias=True),\n","    tt.CenterCrop(img_size),\n","    tt.ToDtype(dtype=torch.float32, scale=True),\n","    tt.Normalize(mean=mean, std=std)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THQRCKVe-o4n","trusted":true},"outputs":[],"source":["# В теории можно усреднять стиль по нескольким изображениям\n","urls = [style_image_url]\n","style_dataset = StyleDataset(urls)\n","content_dataset = ContentDataset(coco_dataset_path, transforms=transforms2)\n","\n","style_loader = DataLoader(style_dataset, batch_size=1)\n","content_loader = DataLoader(content_dataset, batch_size=4, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Monitoring Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z__HHQzQC_nb","trusted":true},"outputs":[],"source":["def denorm_and_permute(img_tensors):\n","    new_image = img_tensors.permute(1, 2, 0) * torch.tensor(std) + torch.tensor(mean)\n","    return new_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uyf1hEttCW8F","trusted":true},"outputs":[],"source":["def show_imgs(images):\n","    # display.clear_output(wait=True)\n","    plt.figure(figsize=(15,8))\n","    for i, img in enumerate(images):\n","        plt.subplot(1, len(images), i + 1)\n","        plt.imshow(denorm_and_permute(img))\n","        plt.axis('off')\n","        plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Watcher():\n","    def __init__(self, rows=2, columns=10):\n","        self.max_img_numbers = rows * columns\n","        self.fig, self.axes = plt.subplots(rows, columns, figsize=(20,5))\n","        \n","        try:\n","            self.axes = self.axes.flat\n","        except:\n","            self.axes = [self.axes]\n","        \n","        for ax in self.axes:\n","            ax.set_visible(False)\n","\n","        self.dh = display.display(self.fig, display_id=True)\n","\n","\n","    def show(self, images_cpu, labels=None, img_numbers=None, message=None):\n","        img_number = min(img_numbers, len(images_cpu)) if img_numbers is not None else len(images_cpu)\n","        img_number = min(img_number, self.max_img_numbers)\n","\n","        if message is not None:\n","            self.fig.suptitle(message)\n","\n","        for i in range(img_number):\n","            self.axes[i].set_visible(True)\n","            self.axes[i].clear()\n","            self.axes[i].imshow(denorm_and_permute(images_cpu[i]))\n","            self.axes[i].axis('off')\n","\n","        self.dh.update(self.fig)\n","        plt.close()\n"]},{"cell_type":"markdown","metadata":{"id":"tiEEhkVS8wPz"},"source":["# Style and Content Feature Extraction\n"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Extractor class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9is37Z0Fgv4","outputId":"dfbe93d2-c11c-410d-a95a-0eef351b9911","trusted":true},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vgg16 = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n","vgg16 = vgg16.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QbbCeyLLGIhg","trusted":true},"outputs":[],"source":["def get_gram(x):\n","    N, c, h, w = x.size()\n","    x = x.view(N, c, h*w)\n","    y = x.transpose(1, 2)\n","    return torch.bmm(x, y) / (c * h * w)\n","\n","\n","class FeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super(FeatureExtractor, self).__init__()\n","        self._image_mean = torch.tensor([0.485, 0.456, 0.406], device=device)\n","        self._image_std = torch.tensor([0.229, 0.224, 0.225], device=device)\n","\n","        self.layers = vgg16.features\n","\n","        for param in self.layers.parameters():\n","            param.requires_grad = False\n","\n","    def extract_features(self, images, active_style_layers=[1, 1, 1, 1], active_content_layers=[0, 0, 1, 0]):\n","        style_features = []\n","        content_features = []\n","        used_layers_number = 0\n","        \n","        for i, layer in enumerate(self.layers):\n","            if used_layers_number == 4:\n","                break\n","\n","            images = layer(images)\n","\n","            if i in [3, 8, 15, 22]:\n","                n, c, h, w = images.size()\n","            \n","                if active_content_layers[used_layers_number] != 0:\n","                    content_features.append(images)\n","                else:\n","                    content_features.append(torch.zeros(1, device=device))\n","\n","                if active_style_layers[used_layers_number] != 0:\n","                    style_feature = get_gram(images)\n","                    style_features.append(style_feature)\n","                else:\n","                    style_features.append(torch.zeros(1, device=device))\n","                \n","                used_layers_number += 1\n","\n","        return content_features, style_features\n","\n","    def extract_style_features(self, images, active_style_layers=[1, 1, 1, 1]):\n","        return self.extract_features(\n","            images, \n","            active_style_layers=active_style_layers, \n","            active_content_layers=[0,0,0,0])[1]\n","\n","    def extract_content_features(self, images, active_content_layers=[0, 0, 1, 0]):\n","        return self.extract_features(\n","            images, \n","            active_style_layers=[0,0,0,0], \n","            active_content_layers=active_content_layers)[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["feature_extractor = FeatureExtractor().to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Reference Style Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["style_features_mean = [torch.tensor([], device=device) for i in range(4)]\n","count = 0\n","\n","for style_image in tqdm(style_loader):\n","    count += 1\n","    style_image = style_image.to(device)\n","#     (C, H, W) -> [(C1, C1), (C2, C2), (C3, C3), (C4, C4), (C5, C5)]\n","    style_feature = feature_extractor.extract_style_features(style_image)\n","    for i in range(4):\n","        if style_features_mean[i].size()[0] == 0:\n","            style_features_mean[i] = style_feature[i].detach()\n","        else:\n","            style_features_mean[i] += style_feature[i].detach()\n","\n","for i in range(4):\n","    style_features_mean[i] /= count"]},{"cell_type":"markdown","metadata":{},"source":["# Loss Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYaxB6d8KbsH","trusted":true},"outputs":[],"source":["def content_loss_fn(generated_content_features, content_features, content_wl):\n","    content_loss = torch.zeros(1, device=device)\n","\n","    for i in range(len(content_features)):\n","        if i >= len(content_wl) or content_wl[i] == 0:\n","            continue\n","    \n","        content_loss += nn.functional.mse_loss(content_features[i], generated_content_features[i]) * content_wl[i]\n","\n","    return content_loss\n","\n","def style_loss_fn(generated_style_features, style_features, style_wl):\n","    style_loss = torch.zeros(1, device=device)\n","    batch_size = generated_style_features[0].size(0)\n","\n","    for i in range(len(style_features)):\n","        if style_wl[i] == 0:\n","            continue\n","\n","        style_loss += torch.sum(torch.linalg.matrix_norm(generated_style_features[i] - style_features[i].tile((batch_size, 1, 1)))**2) * style_wl[i]\n","    return style_loss\n","        \n","def loss_fn_extended(generated_image, generated_image_features, content_features, content_wl, style_features, style_wl, alpha=1e-3):\n","\n","    content_loss = content_loss_fn(generated_image_features[0], content_features, content_wl)\n","    style_loss = style_loss_fn(generated_image_features[1], style_features, style_wl)\n","\n","    return style_loss + alpha * content_loss, style_loss.item(), alpha * content_loss.item()"]},{"cell_type":"markdown","metadata":{},"source":["# Getting the expected Style's Representation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def show_representation(reference_style_feature, loss_fn, iterations=500, wl=[1, 1, 1, 1]):\n","    torch.cuda.empty_cache()\n","    \n","    style_wl = torch.tensor(wl, device=device)\n","    \n","    image = torch.rand((1, 3, 512, 512), device=device, requires_grad=True)\n","    optimizer = torch.optim.Adam([image], lr=8e-2)\n","    \n","    watcher = Watcher(columns=1, rows=1)\n","\n","    for iteration in tqdm(range(iterations)):\n","        optimizer.zero_grad()\n","                        \n","        style_features = feature_extractor.extract_style_features(image)\n","        \n","        loss = loss_fn(style_features, reference_style_feature, style_wl)\n","\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if iteration % 100 == 0:\n","            watcher.show([image[0].detach().cpu()], message=f\"loss: {loss.item()}\")\n","            \n","    return image[0].detach().cpu()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image = show_representation(style_features_mean, style_loss_fn, iterations=5000)\n","img = denorm_and_permute(image)\n","img = (img * 255).permute(2, 0, 1).clamp(0, 255).to(dtype=torch.uint8)\n","torchvision.io.write_jpeg(img, style_representation_file_path)"]},{"cell_type":"markdown","metadata":{},"source":["# Style Transfer Model"]},{"cell_type":"markdown","metadata":{},"source":["## Residual Block"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super(ResidualBlock, self).__init__()\n","        \n","        self.conv0 = nn.Sequential(\n","            nn.Conv2d(channels, channels, kernel_size=3, padding='same'),\n","            nn.ReLU(inplace=True)\n","        )\n","        \n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding='same')\n","        \n","    def forward(self, x):\n","        y = self.conv0(x)\n","        y = self.conv1(y)\n","        x = x + y\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## Convolution and Deconvolution layers"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ConvLayer(nn.Module):\n","    def __init__(self, in_dim, out_dim, kernel_size, stride, norm = \"instance\"):\n","        super(ConvLayer, self).__init__()\n","        padding_size = kernel_size // 2\n","        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n","\n","        self.conv_layer = nn.Conv2d(in_dim, out_dim, kernel_size, stride)\n","\n","        if norm == \"instance\":\n","            self.norm_layer = nn.InstanceNorm2d(out_dim, affine = True)\n","        else:\n","            self.morn_layer = nn.Identity()\n","\n","    def forward(self, x):\n","        x = self.reflection_pad(x)\n","        x = self.conv_layer(x)\n","        out = self.norm_layer(x)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class DeconvLayer(nn.Module):\n","    def __init__(self, in_dim, out_dim, kernel_size, stride, output_padding, norm=\"instance\"):\n","        super(DeconvLayer, self).__init__()\n","\n","        # Transposed Convolution\n","        padding_size = kernel_size // 2\n","        self.conv_transpose = nn.ConvTranspose2d(in_dim, out_dim, kernel_size, stride, padding_size, output_padding)\n","\n","        # Normalization Layers\n","        if (norm == \"instance\"):\n","            self.norm_layer = nn.InstanceNorm2d(out_dim, affine = True)\n","        else:\n","            self.norm_layer = nn.Identity()\n","\n","    def forward(self, x):\n","        x = self.conv_transpose(x)\n","        out = self.norm_layer(x)\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class StyleTransferModel(nn.Module):\n","    def __init__(self):\n","        super(StyleTransferModel, self).__init__()\n","\n","        self.conv0 = nn.Sequential(\n","            ConvLayer(3, 32, 9, 1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.conv1 = nn.Sequential(\n","            ConvLayer(32, 64, 3, 2),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.conv2 = nn.Sequential(\n","            ConvLayer(64, 128, 3, 2),\n","            nn.ReLU(inplace=True)\n","        )\n","        \n","        \n","        self.residuals = nn.ModuleList([ResidualBlock(128) for i in range(5)])\n","        \n","        self.deconv0 = nn.Sequential(\n","            DeconvLayer(128, 64, 3, 2, 1),\n","            nn.ReLU(inplace=True),\n","        )\n","        \n","        self.deconv1 = nn.Sequential(\n","            DeconvLayer(64, 32, 3, 2, 1),\n","            nn.ReLU(inplace=True),\n","        )\n","        \n","        self.deconv2 = nn.Sequential(\n","            DeconvLayer(32, 3, 9, 1, 0, norm=None),\n","            nn.Tanh()\n","        )\n","        \n","    def forward(self, x):\n","        x = self.conv0(x)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","\n","        for residual_layer in self.residuals:\n","            x = residual_layer(x)\n","            \n","        x = self.deconv0(x)\n","        x = self.deconv1(x)\n","        x = self.deconv2(x)\n","            \n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDNxi8jY9hGV","trusted":true},"outputs":[],"source":["def train(\n","    model, \n","    content_loader, \n","    reference_style_feature, \n","    loss_fn, \n","    lr=1e-3, \n","    alpha=1e-3, \n","    content_wl_list=[0, 0, 1, 0], \n","    style_wl_list=[1/4, 1/4, 1/4, 1/4],\n","    file_name=\"model.pt\",\n","    watch=False,\n","    save_model=False\n","):\n","    torch.cuda.empty_cache()\n","\n","    content_wl = torch.tensor(content_wl_list, device=device)\n","    style_wl = torch.tensor(style_wl_list, device=device)\n","    \n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    \n","    iteration = 0\n","    \n","    watcher=Watcher()\n","        \n","    for batch in tqdm(content_loader):\n","        torch.cuda.empty_cache()\n","\n","        model.train()\n","\n","        optimizer.zero_grad()\n","        \n","        batch = batch.to(device)\n","        \n","        with torch.no_grad():\n","            content_features = feature_extractor.extract_content_features(batch, active_content_layers=content_wl_list)\n","            \n","        content_features = [t.detach() for t in content_features]\n","                \n","        batch = batch #+ torch.rand_like(batch)\n","        batch = model.forward(batch)\n","\n","        batch_features = feature_extractor.extract_features(batch, active_style_layers=style_wl_list, active_content_layers=content_wl_list)\n","        \n","        loss, style_loss, content_loss = loss_fn(\n","            batch, \n","            batch_features, \n","            content_features, \n","            content_wl, \n","            reference_style_feature, \n","            style_wl,\n","            alpha=alpha\n","        )\n","\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if iteration % 100 == 0 and watch:\n","            model.eval()\n","            torch.cuda.empty_cache()\n","            with torch.no_grad():\n","                fixed_batch = next(iter(content_loader))[0:3].to(device)\n","                fixed_result = model(fixed_batch)\n","\n","            watcher.show([*fixed_batch.cpu(), *fixed_result.detach().cpu()], message=f\"content loss: {content_loss / len(batch)}, style loss: {style_loss / len(batch)}\")\n","        \n","            del fixed_result, fixed_batch\n","            \n","        if iteration % 500 == 0 and save_model:\n","            torch.save(model, file_name)\n","        \n","        iteration += 1\n","        \n","        "]},{"cell_type":"markdown","metadata":{},"source":["# Training process"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = StyleTransferModel().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yiItJteSQGtU","outputId":"b633b76b-cfd8-4405-9fa9-222a4a954ea4","trusted":true},"outputs":[],"source":["train(\n","    model, \n","    content_loader, \n","    style_features_mean,\n","    loss_fn_extended, \n","    lr=1e-3,\n","#     good value is +-1e-2\n","    alpha=3e-2,\n","    content_wl_list=[0, 0, 1, 0], \n","    style_wl_list=[1/4, 1/4, 1/4, 1/4],\n","    file_name=\"model.pt\",\n","    watch=False\n",")\n",";"]},{"cell_type":"markdown","metadata":{},"source":["# Model saving"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.cpu()\n","model.eval()\n","\n","model_input = torch.randn(1, 3, 256, 256, dtype=torch.float32)\n","torch.onnx.export(\n","    model,\n","    model_input,\n","    model_file_path,\n","    input_names=[\"image\"],\n","    output_names=[\"result\"],\n","    dynamic_axes={\n","        \"image\": {0: \"batch_size\", 2: \"image_height\", 3: \"image_width\"},\n","        \"result\": {0: \"batch_size\", 2: \"image_height\", 3: \"image_width\"},\n","    }\n",")"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":857191,"sourceId":1462296,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"062343b282814973bd805e496899841b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f651d813fb642c88cd97a7a60c51110":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"26083a1a61be4b80b8779f7b01ea482b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e796be84b704b22b2a6a584ca131438":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75ba483c8e6542449f50e403117bedee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8040c33faa2b429fb7dfcfba7f47fb6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_062343b282814973bd805e496899841b","placeholder":"​","style":"IPY_MODEL_26083a1a61be4b80b8779f7b01ea482b","value":" 3000/3000 [01:52&lt;00:00, 26.96it/s]"}},"85ae0806854d420686827711de712e35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a0410ebb7254a8f8b0ba88ee4708cb7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae2ab18adf584c3fb459352c397a105e","IPY_MODEL_8f6b531dafbe4a4e97cf4d0dfa79ebac","IPY_MODEL_8040c33faa2b429fb7dfcfba7f47fb6d"],"layout":"IPY_MODEL_75ba483c8e6542449f50e403117bedee"}},"8f6b531dafbe4a4e97cf4d0dfa79ebac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f45205c4a748419e92b1c2e0f8016235","max":3000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1f651d813fb642c88cd97a7a60c51110","value":3000}},"ae2ab18adf584c3fb459352c397a105e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85ae0806854d420686827711de712e35","placeholder":"​","style":"IPY_MODEL_6e796be84b704b22b2a6a584ca131438","value":"Training iteration: 100%"}},"f45205c4a748419e92b1c2e0f8016235":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":4}
